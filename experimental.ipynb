{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "# from pegasus_dataset import PegasusDataset\n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model_name = \"google/pegasus-large\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cnn_dailymail (/home/jvidakovic/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd8f6aec75214621b6d974441c3d071a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")  # using the unanonimyzed version"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "{'article': Value(dtype='string', id=None),\n 'highlights': Value(dtype='string', id=None),\n 'id': Value(dtype='string', id=None)}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "                                             article  \\\n0  It's official: U.S. President Barack Obama wan...   \n1  (CNN) -- Usain Bolt rounded off the world cham...   \n2  Kansas City, Missouri (CNN) -- The General Ser...   \n3  Los Angeles (CNN) -- A medical doctor in Vanco...   \n4  (CNN) -- Police arrested another teen Thursday...   \n\n                                          highlights  \\\n0  Syrian official: Obama climbed to the top of t...   \n1  Usain Bolt wins third gold of world championsh...   \n2  The employee in agency's Kansas City office is...   \n3  NEW: A Canadian doctor says she was part of a ...   \n4  Another arrest made in gang rape outside Calif...   \n\n                                         id  \n0  0001d1afc246a7964130f43ae940af6bc6c57f01  \n1  0002095e55fcbd3a2f366d9bf92a95433dc305ef  \n2  00027e965c8264c35cc1bc55556db388da82b07f  \n3  0002c17436637c4fe1837c935c04de47adb18e9a  \n4  0003ad6ef0c37534f80b55b4235108024b407f0b  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article</th>\n      <th>highlights</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>It's official: U.S. President Barack Obama wan...</td>\n      <td>Syrian official: Obama climbed to the top of t...</td>\n      <td>0001d1afc246a7964130f43ae940af6bc6c57f01</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(CNN) -- Usain Bolt rounded off the world cham...</td>\n      <td>Usain Bolt wins third gold of world championsh...</td>\n      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Kansas City, Missouri (CNN) -- The General Ser...</td>\n      <td>The employee in agency's Kansas City office is...</td>\n      <td>00027e965c8264c35cc1bc55556db388da82b07f</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Los Angeles (CNN) -- A medical doctor in Vanco...</td>\n      <td>NEW: A Canadian doctor says she was part of a ...</td>\n      <td>0002c17436637c4fe1837c935c04de47adb18e9a</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(CNN) -- Police arrested another teen Thursday...</td>\n      <td>Another arrest made in gang rape outside Calif...</td>\n      <td>0003ad6ef0c37534f80b55b4235108024b407f0b</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dataset[\"train\"].to_dict())\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 287113 entries, 0 to 287112\n",
      "Data columns (total 2 columns):\n",
      " #   Column            Non-Null Count   Dtype\n",
      "---  ------            --------------   -----\n",
      " 0   n_tokens_article  287113 non-null  int64\n",
      " 1   n_tokens_summary  287113 non-null  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 4.4 MB\n"
     ]
    }
   ],
   "source": [
    "df[\"n_tokens_article\"] = df.apply(lambda row: len(row.article.split()), axis=1)\n",
    "df[\"n_tokens_summary\"] = df.apply(lambda row: len(row.highlights.split()), axis=1)\n",
    "df.loc[:, [\"n_tokens_article\", \"n_tokens_summary\"]].info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "       n_tokens_article  n_tokens_summary\ncount     287113.000000     287113.000000\nmean         691.870650         51.574101\nstd          336.500247         21.256336\nmin            8.000000          4.000000\n25%          443.000000         38.000000\n50%          632.000000         48.000000\n75%          877.000000         60.000000\nmax         2347.000000       1296.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_tokens_article</th>\n      <th>n_tokens_summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>287113.000000</td>\n      <td>287113.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>691.870650</td>\n      <td>51.574101</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>336.500247</td>\n      <td>21.256336</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>8.000000</td>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>443.000000</td>\n      <td>38.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>632.000000</td>\n      <td>48.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>877.000000</td>\n      <td>60.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2347.000000</td>\n      <td>1296.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, [\"n_tokens_article\", \"n_tokens_summary\"]].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_texts, train_labels = dataset[\"train\"][\"article\"][:1000], dataset[\"train\"][\"highlights\"][:1000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/1.82M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba8bb4c274bb4370ad5cbdcda7fabe84"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/65.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ba193a019354df7a0061c9e5126a955"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/88.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1942f8d829c4d1fb63dc3e2acf1b6e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/3.02k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f497e508ad2541f78473faa7faf6ae00"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "PreTrainedTokenizer(name_or_path='google/pegasus-large', vocab_size=96103, model_max_len=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask_2>', 'additional_special_tokens': ['<mask_1>', '<unk_2>', '<unk_3>', '<unk_4>', '<unk_5>', '<unk_6>', '<unk_7>', '<unk_8>', '<unk_9>', '<unk_10>', '<unk_11>', '<unk_12>', '<unk_13>', '<unk_14>', '<unk_15>', '<unk_16>', '<unk_17>', '<unk_18>', '<unk_19>', '<unk_20>', '<unk_21>', '<unk_22>', '<unk_23>', '<unk_24>', '<unk_25>', '<unk_26>', '<unk_27>', '<unk_28>', '<unk_29>', '<unk_30>', '<unk_31>', '<unk_32>', '<unk_33>', '<unk_34>', '<unk_35>', '<unk_36>', '<unk_37>', '<unk_38>', '<unk_39>', '<unk_40>', '<unk_41>', '<unk_42>', '<unk_43>', '<unk_44>', '<unk_45>', '<unk_46>', '<unk_47>', '<unk_48>', '<unk_49>', '<unk_50>', '<unk_51>', '<unk_52>', '<unk_53>', '<unk_54>', '<unk_55>', '<unk_56>', '<unk_57>', '<unk_58>', '<unk_59>', '<unk_60>', '<unk_61>', '<unk_62>', '<unk_63>', '<unk_64>', '<unk_65>', '<unk_66>', '<unk_67>', '<unk_68>', '<unk_69>', '<unk_70>', '<unk_71>', '<unk_72>', '<unk_73>', '<unk_74>', '<unk_75>', '<unk_76>', '<unk_77>', '<unk_78>', '<unk_79>', '<unk_80>', '<unk_81>', '<unk_82>', '<unk_83>', '<unk_84>', '<unk_85>', '<unk_86>', '<unk_87>', '<unk_88>', '<unk_89>', '<unk_90>', '<unk_91>', '<unk_92>', '<unk_93>', '<unk_94>', '<unk_95>', '<unk_96>', '<unk_97>', '<unk_98>', '<unk_99>', '<unk_100>', '<unk_101>', '<unk_102>']})"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from transformers import BatchEncoding\n",
    "\n",
    "train_input_encodings: BatchEncoding = tokenizer(train_texts, truncation=True, padding=True)\n",
    "train_output_encodings: BatchEncoding = tokenizer(train_labels, truncation=True, padding=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "transformers.tokenization_utils_base.BatchEncoding"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_input_encodings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PegasusDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        # TODO - check what this is\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])  # torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels['input_ids'])  # len(self.labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def prepare_data(model_name,\n",
    "                 train_texts, train_labels,\n",
    "                 val_texts=None, val_labels=None,\n",
    "                 test_texts=None, test_labels=None):\n",
    "    \"\"\"\n",
    "    Prepare input data for model fine-tuning\n",
    "    \"\"\"\n",
    "\n",
    "    # this should be a sentencepiece tokenizer\n",
    "    tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    prepare_val = False if val_texts is None or val_labels is None else True\n",
    "    prepare_test = False if test_texts is None or test_labels is None else True\n",
    "\n",
    "    def tokenize_data(texts, labels):\n",
    "        encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "        decodings = tokenizer(labels, truncation=True, padding=True)\n",
    "        dataset_tokenized = PegasusDataset(encodings, decodings)\n",
    "        return dataset_tokenized\n",
    "\n",
    "    train_dataset = tokenize_data(train_texts, train_labels)\n",
    "    val_dataset = tokenize_data(val_texts, val_labels) if prepare_val else None\n",
    "    test_dataset = tokenize_data(test_texts, test_labels) if prepare_test else None\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, tokenizer\n",
    "\n",
    "\n",
    "def prepare_fine_tuning(model_name, tokenizer, train_dataset, val_dataset=None, freeze_encoder=False,\n",
    "                        output_dir='./results'):\n",
    "    \"\"\"\n",
    "    Prepare configurations and base model for fine-tuning\n",
    "    \"\"\"\n",
    "    torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "    if freeze_encoder:\n",
    "        for param in model.model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    if val_dataset is not None:\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,  # output directory\n",
    "            num_train_epochs=10,  # total number of training epochs\n",
    "            per_device_train_batch_size=1,  # batch size per device during training, can increase if memory allows\n",
    "            per_device_eval_batch_size=1,  # batch size for evaluation, can increase if memory allows\n",
    "            save_steps=500,  # number of updates steps before checkpoint saves\n",
    "            save_total_limit=5,  # limit the total amount of checkpoints and deletes the older checkpoints\n",
    "            evaluation_strategy='steps',  # evaluation strategy to adopt during training\n",
    "            eval_steps=100,  # number of update steps before evaluation\n",
    "            warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
    "            weight_decay=0.01,  # strength of weight decay\n",
    "            logging_dir='./logs',  # directory for storing logs\n",
    "            logging_steps=10,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,  # the instantiated 🤗 Transformers model to be trained\n",
    "            args=training_args,  # training arguments, defined above\n",
    "            train_dataset=train_dataset,  # training dataset\n",
    "            eval_dataset=val_dataset,  # evaluation dataset\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,  # output directory\n",
    "            num_train_epochs=10,  # total number of training epochs\n",
    "            per_device_train_batch_size=1,  # batch size per device during training, can increase if memory allows\n",
    "            save_steps=500,  # number of updates steps before checkpoint saves\n",
    "            save_total_limit=5,  # limit the total amount of checkpoints and deletes the older checkpoints\n",
    "            warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
    "            weight_decay=0.01,  # strength of weight decay\n",
    "            logging_dir='./logs',  # directory for storing logs\n",
    "            logging_steps=10,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,  # the instantiated 🤗 Transformers model to be trained\n",
    "            args=training_args,  # training arguments, defined above\n",
    "            train_dataset=train_dataset,  # training dataset\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "    return trainer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_dataset, _, _, tokenizer = prepare_data(model_name, train_texts, train_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "trainer = prepare_fine_tuning(model_name, tokenizer, train_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jvidakovic/.conda/envs/seminar-2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5000\n",
      "/home/jvidakovic/.conda/envs/seminar-2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/5000 : < :, Epoch 0.00/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
      "/home/jvidakovic/.conda/envs/seminar-2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
      "/home/jvidakovic/.conda/envs/seminar-2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./results/checkpoint-1500\n",
      "Configuration saved in ./results/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1500/special_tokens_map.json\n",
      "/home/jvidakovic/.conda/envs/seminar-2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
